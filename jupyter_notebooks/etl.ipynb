{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **(ADD THE NOTEBOOK NAME HERE)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "Create a single, analysis-ready dataset by:\n",
        "\n",
        "* Loading and cleaning the two raw CSVs from the Global Food Price Inflation 2024 Kaggle archive\n",
        "\n",
        "* Standardising column names, parsing dates, and coercing numeric fields\n",
        "\n",
        "* Harmonising key categorical fields (e.g., country, item)\n",
        "\n",
        "* Merging item-/series-level details with country-level context using reliable keys\n",
        "\n",
        "* Saving cleaned per-file outputs and a merged dataset for downstream analysis\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* data/raw/WLD_RTFP_country_2023-10-02.csv\n",
        "\n",
        "* data/raw/WLD_RTP_details_2023-10-02.csv\n",
        "\n",
        "* Python packages: pandas, numpy, os, re, pathlib\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* data/processed/WLD_RTFP_country_2023-10-02_clean.csv\n",
        "\n",
        "* data/processed/WLD_RTP_details_2023-10-02_clean.csv\n",
        "\n",
        "* data/processed/food_price_merged_clean.csv (final, analysis-ready)\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* Join strategy: left join from details â†’ country, prioritising keys ['country','date','item'] then ['country','date'], falling back to ['country'] only if needed.\n",
        "\n",
        "* If your columns differ slightly (e.g., country_name instead of country, month instead of date), the notebook will adapt and log exactly which keys were used.\n",
        "\n",
        "* Currency/unit normalisation can be added as a follow-up step once business rules are agreed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We assume this notebook sits in a subfolder (e.g., jupyter_notebooks/). We make the parent the working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/aminaibrahim/Documents/vscode-projects/food-price-inflation-analysis/jupyter_notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Access the current directory\n",
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "# Make the parent directory current\n",
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/aminaibrahim/Documents/vscode-projects/food-price-inflation-analysis'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Confirm new current directory\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 1 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "country_path = \"data/WLD_RTFP_country_2023-10-02.csv\"\n",
        "details_path = \"data/WLD_RTP_details_2023-10-02.csv\"\n",
        "\n",
        "country_df = pd.read_csv(country_path)\n",
        "details_df = pd.read_csv(details_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Utility Functions\n",
        "\n",
        "The following code defines a set of reusable functions to clean and standardize tabular data using pandas. These functions help ensure consistent column naming, tidy string values, parse dates, convert numeric columns, drop nearly empty columns, and provide a quick summary report. You can use these utilities to prepare raw datasets for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Standardize column names: strip spaces, remove special characters, replace spaces with underscores, and lowercase\n",
        "def normalise_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df.columns = (\n",
        "        df.columns\n",
        "          .str.strip()\n",
        "          .str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
        "          .str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "          .str.lower()\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# Clean string columns: strip leading/trailing spaces and collapse multiple spaces into one\n",
        "def tidy_strings(df: pd.DataFrame, cols) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = (df[c].astype(str)\n",
        "                           .str.strip()\n",
        "                           .str.replace(r\"\\s+\", \" \", regex=True))\n",
        "    return df\n",
        "\n",
        "# Parse columns with names like 'date', 'month', 'year', or 'period' into datetime objects\n",
        "def parse_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if re.search(r\"(date|month|year|period)\", c):\n",
        "            try:\n",
        "                df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
        "            except Exception:\n",
        "                pass\n",
        "    return df\n",
        "\n",
        "# Convert columns that look numeric (even if stored as strings) to numeric dtype\n",
        "def coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if df[c].dtype == object:\n",
        "            sample = df[c].dropna().astype(str).head(50)\n",
        "            looks_numeric = (\n",
        "                not sample.empty and\n",
        "                sample.str.replace(\",\",\"\", regex=False)\n",
        "                      .str.replace(\"%\",\"\", regex=False)\n",
        "                      .str.match(r\"^-?\\d+(\\.\\d+)?$\")\n",
        "                      .mean() > 0.6\n",
        "            )\n",
        "            if looks_numeric:\n",
        "                df[c] = (df[c].astype(str)\n",
        "                               .str.replace(\",\",\"\", regex=False)\n",
        "                               .str.replace(\"%\",\"\", regex=False)\n",
        "                               .str.strip())\n",
        "                df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "# Drop columns that are nearly empty (default: 98% or more missing values)\n",
        "def drop_nearly_empty(df: pd.DataFrame, thresh=0.98) -> pd.DataFrame:\n",
        "    na_ratio = df.isna().mean()\n",
        "    to_drop = na_ratio[na_ratio >= thresh].index.tolist()\n",
        "    return df.drop(columns=to_drop) if to_drop else df\n",
        "\n",
        "# Run all cleaning steps in sequence for a generic DataFrame\n",
        "def clean_generic(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    out = normalise_columns(out)\n",
        "    out = out.drop_duplicates()\n",
        "    # Identify likely categorical columns and clean them\n",
        "    cat_cols = [c for c in out.columns if out[c].dtype == object]\n",
        "    priority = [c for c in [\"country\",\"country_name\",\"item\",\"commodity\",\"product\",\"unit\",\"currency\",\"series\",\"market\"] if c in out.columns]\n",
        "    out = tidy_strings(out, list(dict.fromkeys(priority + cat_cols)))\n",
        "    for c in [\"country\",\"country_name\"]:\n",
        "        if c in out.columns:\n",
        "            out[c] = out[c].str.title()\n",
        "    out = parse_dates(out)\n",
        "    out = coerce_numeric(out)\n",
        "    out = drop_nearly_empty(out, thresh=0.98)\n",
        "    return out\n",
        "\n",
        "# Print a brief summary report of the DataFrame's shape, duplicate count, and top missing columns\n",
        "def brief_report(df: pd.DataFrame, name: str):\n",
        "    print(f\"{name}: shape={df.shape}, duplicates={df.duplicated().sum()}\")\n",
        "    miss = df.isna().sum()\n",
        "    if miss.any():\n",
        "        print(\"  top missing:\", miss.sort_values(ascending=False).head(5).to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying Data Cleaning Functions\n",
        "\n",
        "Now we use the cleaning utilities to process both datasets (`country_df` and `details_df`).  \n",
        "We then print a brief summary report for each cleaned DataFrame to check their shape, duplicate count, and missing values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In cases where you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "try:\n",
        "  # create your folder here\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
