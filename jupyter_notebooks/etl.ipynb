{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **(ADD THE NOTEBOOK NAME HERE)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "Create a single, analysis-ready dataset by:\n",
        "\n",
        "* Loading and cleaning the two raw CSVs from the Global Food Price Inflation 2024 Kaggle archive\n",
        "\n",
        "* Standardising column names, parsing dates, and coercing numeric fields\n",
        "\n",
        "* Harmonising key categorical fields (e.g., country, item)\n",
        "\n",
        "* Merging item-/series-level details with country-level context using reliable keys\n",
        "\n",
        "* Saving cleaned per-file outputs and a merged dataset for downstream analysis\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* data/raw/WLD_RTFP_country_2023-10-02.csv\n",
        "\n",
        "* data/raw/WLD_RTP_details_2023-10-02.csv\n",
        "\n",
        "* Python packages: pandas, numpy, os, re, pathlib\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* data/processed/WLD_RTFP_country_2023-10-02_clean.csv\n",
        "\n",
        "* data/processed/WLD_RTP_details_2023-10-02_clean.csv\n",
        "\n",
        "* data/processed/food_price_merged_clean.csv (final, analysis-ready)\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* Join strategy: left join from details â†’ country, prioritising keys ['country','date','item'] then ['country','date'], falling back to ['country'] only if needed.\n",
        "\n",
        "* If your columns differ slightly (e.g., country_name instead of country, month instead of date), the notebook will adapt and log exactly which keys were used.\n",
        "\n",
        "* Currency/unit normalisation can be added as a follow-up step once business rules are agreed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We assume this notebook sits in a subfolder (e.g., jupyter_notebooks/). We make the parent the working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/aminaibrahim/Documents/vscode-projects/food-price-inflation-analysis/jupyter_notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Access the current directory\n",
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "# Make the parent directory current\n",
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/aminaibrahim/Documents/vscode-projects/food-price-inflation-analysis'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Confirm new current directory\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Section 1 content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "country_path = \"data/WLD_RTFP_country_2023-10-02.csv\"\n",
        "details_path = \"data/WLD_RTP_details_2023-10-02.csv\"\n",
        "\n",
        "country_df = pd.read_csv(country_path)\n",
        "details_df = pd.read_csv(details_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Utility Functions\n",
        "\n",
        "The following code defines a set of reusable functions to clean and standardize tabular data using pandas. These functions help ensure consistent column naming, tidy string values, parse dates, convert numeric columns, drop nearly empty columns, and provide a quick summary report. You can use these utilities to prepare raw datasets for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Standardize column names: strip spaces, remove special characters, replace spaces with underscores, and lowercase\n",
        "def normalise_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df.columns = (\n",
        "        df.columns\n",
        "          .str.strip()\n",
        "          .str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
        "          .str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "          .str.lower()\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# Clean string columns: strip leading/trailing spaces and collapse multiple spaces into one\n",
        "def tidy_strings(df: pd.DataFrame, cols) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = (df[c].astype(str)\n",
        "                           .str.strip()\n",
        "                           .str.replace(r\"\\s+\", \" \", regex=True))\n",
        "    return df\n",
        "\n",
        "# Parse columns with names like 'date', 'month', 'year', or 'period' into datetime objects\n",
        "def parse_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if re.search(r\"(date|month|year|period)\", c):\n",
        "            try:\n",
        "                df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
        "            except Exception:\n",
        "                pass\n",
        "    return df\n",
        "\n",
        "# Convert columns that look numeric (even if stored as strings) to numeric dtype\n",
        "def coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if df[c].dtype == object:\n",
        "            sample = df[c].dropna().astype(str).head(50)\n",
        "            looks_numeric = (\n",
        "                not sample.empty and\n",
        "                sample.str.replace(\",\",\"\", regex=False)\n",
        "                      .str.replace(\"%\",\"\", regex=False)\n",
        "                      .str.match(r\"^-?\\d+(\\.\\d+)?$\")\n",
        "                      .mean() > 0.6\n",
        "            )\n",
        "            if looks_numeric:\n",
        "                df[c] = (df[c].astype(str)\n",
        "                               .str.replace(\",\",\"\", regex=False)\n",
        "                               .str.replace(\"%\",\"\", regex=False)\n",
        "                               .str.strip())\n",
        "                df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "# Drop columns that are nearly empty (default: 98% or more missing values)\n",
        "def drop_nearly_empty(df: pd.DataFrame, thresh=0.98) -> pd.DataFrame:\n",
        "    na_ratio = df.isna().mean()\n",
        "    to_drop = na_ratio[na_ratio >= thresh].index.tolist()\n",
        "    return df.drop(columns=to_drop) if to_drop else df\n",
        "\n",
        "# Run all cleaning steps in sequence for a generic DataFrame\n",
        "def clean_generic(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    out = normalise_columns(out)\n",
        "    out = out.drop_duplicates()\n",
        "    # Identify likely categorical columns and clean them\n",
        "    cat_cols = [c for c in out.columns if out[c].dtype == object]\n",
        "    priority = [c for c in [\"country\",\"country_name\",\"item\",\"commodity\",\"product\",\"unit\",\"currency\",\"series\",\"market\"] if c in out.columns]\n",
        "    out = tidy_strings(out, list(dict.fromkeys(priority + cat_cols)))\n",
        "    for c in [\"country\",\"country_name\"]:\n",
        "        if c in out.columns:\n",
        "            out[c] = out[c].str.title()\n",
        "    out = parse_dates(out)\n",
        "    out = coerce_numeric(out)\n",
        "    out = drop_nearly_empty(out, thresh=0.98)\n",
        "    return out\n",
        "\n",
        "# Print a brief summary report of the DataFrame's shape, duplicate count, and top missing columns\n",
        "def brief_report(df: pd.DataFrame, name: str):\n",
        "    print(f\"{name}: shape={df.shape}, duplicates={df.duplicated().sum()}\")\n",
        "    miss = df.isna().sum()\n",
        "    if miss.any():\n",
        "        print(\"  top missing:\", miss.sort_values(ascending=False).head(5).to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying Data Cleaning Functions\n",
        "\n",
        "Now we use the cleaning utilities to process both datasets (`country_df` and `details_df`).  \n",
        "We then print a brief summary report for each cleaned DataFrame to check their shape, duplicate count, and missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Country (clean): shape=(4798, 8), duplicates=0\n",
            "  top missing: {'inflation': 364, 'open': 64, 'high': 64, 'low': 64, 'close': 64}\n",
            "Details (clean): shape=(25, 21), duplicates=0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/09/yd5kcn1539g31p80cnw4dmr80000gn/T/ipykernel_39963/3821305012.py:33: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
            "/var/folders/09/yd5kcn1539g31p80cnw4dmr80000gn/T/ipykernel_39963/3821305012.py:33: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
            "/var/folders/09/yd5kcn1539g31p80cnw4dmr80000gn/T/ipykernel_39963/3821305012.py:33: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
            "/var/folders/09/yd5kcn1539g31p80cnw4dmr80000gn/T/ipykernel_39963/3821305012.py:33: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n"
          ]
        }
      ],
      "source": [
        "# Clean both datasets using the generic cleaning function\n",
        "country_clean = clean_generic(country_df)\n",
        "details_clean = clean_generic(details_df)\n",
        "\n",
        "# Print a summary report for each cleaned DataFrame\n",
        "brief_report(country_clean, \"Country (clean)\")\n",
        "brief_report(details_clean, \"Details (clean)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 4 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving Cleaned Data\n",
        "\n",
        "After cleaning, we save the processed DataFrames as new CSV files in the `data/` directory.  \n",
        "This ensures that the cleaned datasets are available for downstream analysis or sharing, without overwriting the original raw files.  \n",
        "The file names include `_clean` to indicate they have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: data/WLD_RTFP_country_2023-10-02_clean.csv\n",
            "Saved: data/WLD_RTP_details_2023-10-02_clean.csv\n"
          ]
        }
      ],
      "source": [
        "# Save cleaned versions alongside the raw files in 'data/'\n",
        "country_out = \"data/WLD_RTFP_country_2023-10-02_clean.csv\"\n",
        "details_out = \"data/WLD_RTP_details_2023-10-02_clean.csv\"\n",
        "\n",
        "country_clean.to_csv(country_out, index=False)\n",
        "details_clean.to_csv(details_out, index=False)\n",
        "\n",
        "print(\"Saved:\", country_out)\n",
        "print(\"Saved:\", details_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 5 â€” Merge to a single merged cleaned dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merging Cleaned Datasets\n",
        "\n",
        "To combine the cleaned detail and country datasets, we first ensure both have consistent key columns for merging:\n",
        "- `add_country_key` copies the `country_name` column to `country` if `country` is missing.\n",
        "- `ensure_date` creates a `date` column from alternatives like `month`, `period`, or `year` if needed.\n",
        "\n",
        "We then select the best available join keys from a prioritized list (e.g., `[\"country\",\"date\",\"item\"]`), and perform a left merge from the details to the country data.  \n",
        "This approach ensures robust merging even if column names or structures differ slightly between files.  \n",
        "Finally, we print the merge keys used, the shape of the merged DataFrame, and the number of rows with missing `country` after merging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging on keys: ['country']\n",
            "Merged shape: (4798, 28)\n",
            "Rows with missing `country` after merge: 0\n"
          ]
        }
      ],
      "source": [
        "def add_country_key(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if \"country\" not in df.columns and \"country_name\" in df.columns:\n",
        "        df[\"country\"] = df[\"country_name\"]\n",
        "    return df\n",
        "\n",
        "def ensure_date(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if \"date\" not in df.columns:\n",
        "        for alt in [\"month\",\"period\",\"year\"]:\n",
        "            if alt in df.columns and pd.api.types.is_datetime64_any_dtype(df[alt]):\n",
        "                df[\"date\"] = df[alt]\n",
        "                break\n",
        "    return df\n",
        "\n",
        "country_keyed = ensure_date(add_country_key(country_clean))\n",
        "details_keyed = ensure_date(add_country_key(details_clean))\n",
        "\n",
        "candidate_key_sets = [\n",
        "    [\"country\",\"date\",\"item\"],\n",
        "    [\"country\",\"date\"],\n",
        "    [\"country\"]\n",
        "]\n",
        "\n",
        "chosen = None\n",
        "for keys in candidate_key_sets:\n",
        "    if all(k in country_keyed.columns for k in keys) and all(k in details_keyed.columns for k in keys):\n",
        "        chosen = keys\n",
        "        break\n",
        "\n",
        "if not chosen:\n",
        "    raise ValueError(\"No suitable join keys found. Inspect columns and adjust `candidate_key_sets`.\")\n",
        "\n",
        "print(\"Merging on keys:\", chosen)\n",
        "\n",
        "merged = pd.merge(\n",
        "    details_keyed,\n",
        "    country_keyed,\n",
        "    on=chosen,\n",
        "    how=\"left\",\n",
        "    suffixes=(\"\", \"_country\")\n",
        ").drop_duplicates()\n",
        "\n",
        "print(\"Merged shape:\", merged.shape)\n",
        "no_match = merged[\"country\"].isna().sum() if \"country\" in merged.columns else 0\n",
        "print(\"Rows with missing `country` after merge:\", int(no_match))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Country duplicate keys: 4773\n",
            "Details duplicate keys: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for duplicate keys before merging\n",
        "print(\"Country duplicate keys:\", country_keyed.duplicated(subset=chosen).sum())\n",
        "print(\"Details duplicate keys:\", details_keyed.duplicated(subset=chosen).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why I Need to Fix Duplicate Keys Before Merging\n",
        "\n",
        "When I see duplicate keys in my `country_keyed` DataFrame, it means that for some join keys, there are multiple matching rows. If I merge without fixing this, each row in `details_keyed` will join with all matching rows in `country_keyed`, which can blow up my merged dataset and introduce errors.\n",
        "\n",
        "To keep my merged data clean and accurate, I need to investigate and resolve these duplicates in `country_keyed` before merging. This might mean dropping exact duplicates, aggregating values, or deciding which record to keep for each key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Country duplicate keys: 4773\n",
            "Details duplicate keys: 0\n",
            "Missing 'country' in details: 0\n",
            "Missing 'country' in country: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for duplicate keys before merging\n",
        "print(\"Country duplicate keys:\", country_keyed.duplicated(subset=chosen).sum())\n",
        "print(\"Details duplicate keys:\", details_keyed.duplicated(subset=chosen).sum())\n",
        "\n",
        "# Check for missing join keys\n",
        "for k in chosen:\n",
        "    print(f\"Missing '{k}' in details:\", details_keyed[k].isna().sum())\n",
        "    print(f\"Missing '{k}' in country:\", country_keyed[k].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resolving Duplicate Keys Before Merging\n",
        "\n",
        "When I check for duplicate keys before merging, I see that there are 4773 duplicate key rows in my `country_keyed` DataFrame and 0 in `details_keyed`. This means that, for my chosen join keys, the country-level data is not unique. If I merge as-is, each row in `details_keyed` will join with all matching rows in `country_keyed`, which could inflate my merged dataset and introduce errors.\n",
        "\n",
        "Additionally, I see there are no missing values for the 'country' key in either DataFrame, so I don't need to worry about missing join keys.\n",
        "\n",
        "Now that I've identified duplicate keys in my `country_keyed` DataFrame, I need to resolve them before merging. This helps prevent data inflation and ensures the merged dataset is accurate.\n",
        "\n",
        "If the duplicate rows are exact matches, I can safely drop them. If they differ, I'll need to review and decide how to aggregate or select a representative row for each key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Country duplicate keys after cleaning: 0\n"
          ]
        }
      ],
      "source": [
        "# Drop exact duplicate rows based on the chosen join keys\n",
        "country_keyed = country_keyed.drop_duplicates(subset=chosen)\n",
        "\n",
        "# Re-check for duplicates to confirm resolution\n",
        "print(\"Country duplicate keys after cleaning:\", country_keyed.duplicated(subset=chosen).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 6 â€” Save merged cleaned file & quick Quality Assurance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6 â€” Save Merged Cleaned File & Quick Quality Assurance\n",
        "\n",
        "Now that I've resolved duplicate keys and completed the merge, I will save the final cleaned dataset to disk.  \n",
        "I'll also run a quick quality assurance check by summarizing numeric columns and showing the most frequent values for key categorical columns.  \n",
        "This helps ensure the merged data is ready for downstream analysis and gives me a first look at the data distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved merged cleaned dataset: data/food_price_merged_cleaned.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>number_of_markets_modeled</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>57.067945</td>\n",
              "      <td>46.429370</td>\n",
              "      <td>9.00</td>\n",
              "      <td>24.000</td>\n",
              "      <td>42.00</td>\n",
              "      <td>79.00</td>\n",
              "      <td>228.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number_of_markets_covered</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>57.067945</td>\n",
              "      <td>46.429370</td>\n",
              "      <td>9.00</td>\n",
              "      <td>24.000</td>\n",
              "      <td>42.00</td>\n",
              "      <td>79.00</td>\n",
              "      <td>228.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number_of_food_items</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>8.946644</td>\n",
              "      <td>6.629954</td>\n",
              "      <td>3.00</td>\n",
              "      <td>4.000</td>\n",
              "      <td>7.00</td>\n",
              "      <td>11.00</td>\n",
              "      <td>26.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>data_coverage_food</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>34.872926</td>\n",
              "      <td>17.070135</td>\n",
              "      <td>8.84</td>\n",
              "      <td>20.125</td>\n",
              "      <td>31.08</td>\n",
              "      <td>47.97</td>\n",
              "      <td>69.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>average_annualized_food_inflation</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>11.857870</td>\n",
              "      <td>14.511799</td>\n",
              "      <td>1.24</td>\n",
              "      <td>3.580</td>\n",
              "      <td>6.68</td>\n",
              "      <td>10.79</td>\n",
              "      <td>55.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maximum_food_drawdown</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>-23.375060</td>\n",
              "      <td>10.914257</td>\n",
              "      <td>-40.67</td>\n",
              "      <td>-32.670</td>\n",
              "      <td>-23.76</td>\n",
              "      <td>-13.96</td>\n",
              "      <td>-2.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>average_annualized_food_volatility</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>10.614506</td>\n",
              "      <td>5.290399</td>\n",
              "      <td>1.84</td>\n",
              "      <td>7.150</td>\n",
              "      <td>9.89</td>\n",
              "      <td>12.58</td>\n",
              "      <td>24.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>average_annual_food_price_correlation_between_markets</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>0.781647</td>\n",
              "      <td>0.171540</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.670</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index_confidence_score</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>0.919329</td>\n",
              "      <td>0.039085</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.890</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>open</th>\n",
              "      <td>4734.0</td>\n",
              "      <td>1.491880</td>\n",
              "      <td>4.652457</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.740</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.10</td>\n",
              "      <td>102.46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     count       mean  \\\n",
              "number_of_markets_modeled                           4798.0  57.067945   \n",
              "number_of_markets_covered                           4798.0  57.067945   \n",
              "number_of_food_items                                4798.0   8.946644   \n",
              "data_coverage_food                                  4798.0  34.872926   \n",
              "average_annualized_food_inflation                   4798.0  11.857870   \n",
              "maximum_food_drawdown                               4798.0 -23.375060   \n",
              "average_annualized_food_volatility                  4798.0  10.614506   \n",
              "average_annual_food_price_correlation_between_m...  4798.0   0.781647   \n",
              "index_confidence_score                              4798.0   0.919329   \n",
              "open                                                4734.0   1.491880   \n",
              "\n",
              "                                                          std    min     25%  \\\n",
              "number_of_markets_modeled                           46.429370   9.00  24.000   \n",
              "number_of_markets_covered                           46.429370   9.00  24.000   \n",
              "number_of_food_items                                 6.629954   3.00   4.000   \n",
              "data_coverage_food                                  17.070135   8.84  20.125   \n",
              "average_annualized_food_inflation                   14.511799   1.24   3.580   \n",
              "maximum_food_drawdown                               10.914257 -40.67 -32.670   \n",
              "average_annualized_food_volatility                   5.290399   1.84   7.150   \n",
              "average_annual_food_price_correlation_between_m...   0.171540   0.27   0.670   \n",
              "index_confidence_score                               0.039085   0.82   0.890   \n",
              "open                                                 4.652457   0.01   0.740   \n",
              "\n",
              "                                                      50%    75%     max  \n",
              "number_of_markets_modeled                           42.00  79.00  228.00  \n",
              "number_of_markets_covered                           42.00  79.00  228.00  \n",
              "number_of_food_items                                 7.00  11.00   26.00  \n",
              "data_coverage_food                                  31.08  47.97   69.91  \n",
              "average_annualized_food_inflation                    6.68  10.79   55.30  \n",
              "maximum_food_drawdown                              -23.76 -13.96   -2.79  \n",
              "average_annualized_food_volatility                   9.89  12.58   24.77  \n",
              "average_annual_food_price_correlation_between_m...   0.82   0.91    0.99  \n",
              "index_confidence_score                               0.93   0.95    0.99  \n",
              "open                                                 0.96   1.10  102.46  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top values for `country`:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "country\n",
              "Afghanistan    202\n",
              "Somalia        202\n",
              "Nigeria        202\n",
              "Niger          202\n",
              "Mozambique     202\n",
              "Mali           202\n",
              "Liberia        202\n",
              "Burundi        202\n",
              "Lao Pdr        202\n",
              "South Sudan    202\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top values for `currency`:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "currency\n",
              "XOF    808\n",
              "XAF    760\n",
              "AFN    202\n",
              "SOS    202\n",
              "CDF    202\n",
              "GMD    202\n",
              "HTG    202\n",
              "SSP    202\n",
              "LAK    202\n",
              "BIF    202\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Save the merged cleaned dataset\n",
        "merged_out = \"data/food_price_merged_cleaned.csv\"\n",
        "merged.to_csv(merged_out, index=False)\n",
        "print(\"Saved merged cleaned dataset:\", merged_out)\n",
        "\n",
        "# Quick numeric summary (first 10 rows of summary only to keep output tidy)\n",
        "import numpy as np\n",
        "num_summary = merged.select_dtypes(include=[np.number]).describe().T.head(10)\n",
        "display(num_summary)\n",
        "\n",
        "# Frequent categories to guide EDA\n",
        "for key in [\"country\", \"item\", \"unit\", \"currency\", \"series\"]:\n",
        "    if key in merged.columns:\n",
        "        print(f\"\\nTop values for `{key}`:\")\n",
        "        display(merged[key].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 7 â€” Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Now that I have a clean, merged dataset, I can begin feature engineering to create new variables that may improve analysis and modeling. Feature engineering helps extract more information from the data and can reveal important trends or relationships.\n",
        "\n",
        "**Possible feature engineering steps:**\n",
        "- Create time-based features (e.g., year, month, quarter) from date columns.\n",
        "- Calculate percentage change or rolling averages for price or inflation columns.\n",
        "- Encode categorical variables (e.g., one-hot encoding for `country`, `item`, or `unit`).\n",
        "- Create flags for missing or extreme values.\n",
        "- Aggregate data at different levels (e.g., country-level monthly averages).\n",
        "\n",
        "Below, I demonstrate some initial feature engineering steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example feature engineering on the merged dataset\n",
        "\n",
        "# 1. Extract year and month from a 'date' column if present\n",
        "if \"date\" in merged.columns:\n",
        "    merged[\"year\"] = merged[\"date\"].dt.year\n",
        "    merged[\"month\"] = merged[\"date\"].dt.month\n",
        "\n",
        "# 2. Calculate percentage change for a price column if present\n",
        "if \"close\" in merged.columns:\n",
        "    merged = merged.sort_values([\"country\", \"date\"])\n",
        "    merged[\"close_pct_change\"] = merged.groupby(\"country\")[\"close\"].pct_change()\n",
        "\n",
        "# 3. Create a rolling average for 'close' price (window=3 months)\n",
        "if \"close\" in merged.columns:\n",
        "    merged[\"close_rolling_avg_3\"] = merged.groupby(\"country\")[\"close\"].transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
        "\n",
        "# 4. One-hot encode the 'item' column if present\n",
        "if \"item\" in merged.columns:\n",
        "    merged = pd.get_dummies(merged, columns=[\"item\"], prefix=\"item\")\n",
        "\n",
        "# 5. Flag missing values in 'inflation' column\n",
        "if \"inflation\" in merged.columns:\n",
        "    merged[\"inflation_missing\"] = merged[\"inflation\"].isna().astype(int)\n",
        "\n",
        "# Preview the engineered features\n",
        "display(merged.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* You may add as many sections as you want, as long as it supports your project workflow.\n",
        "* All notebook's cells should be run top-down (you can't create a dynamic wherein a given point you need to go back to a previous cell to execute some task, like go back to a previous cell and refresh a variable content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "# Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* In cases where you don't need to push files to Repo, you may replace this section with \"Conclusions and Next Steps\" and state your conclusions and next steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [
        {
          "ename": "IndentationError",
          "evalue": "expected an indented block after 'try' statement on line 2 (553063055.py, line 5)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mexcept Exception as e:\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after 'try' statement on line 2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "try:\n",
        "  # create your folder here\n",
        "  # os.makedirs(name='')\n",
        "except Exception as e:\n",
        "  print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
