{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Food Price analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "Create a single, analysis-ready dataset by:\n",
        "\n",
        "* Loading and cleaning the two raw CSVs from the Global Food Price Inflation 2024 Kaggle archive\n",
        "\n",
        "* Standardising column names, parsing dates, and coercing numeric fields\n",
        "\n",
        "* Harmonising key categorical fields (e.g., country, item)\n",
        "\n",
        "* Merging item-/series-level details with country-level context using reliable keys\n",
        "\n",
        "* Saving cleaned per-file outputs and a merged dataset for downstream analysis\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* data/raw/WLD_RTFP_country_2023-10-02.csv\n",
        "\n",
        "* data/raw/WLD_RTP_details_2023-10-02.csv\n",
        "\n",
        "* Python packages: pandas, numpy, os, re, pathlib\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* data/processed/WLD_RTFP_country_2023-10-02_clean.csv\n",
        "\n",
        "* data/processed/WLD_RTP_details_2023-10-02_clean.csv\n",
        "\n",
        "* data/processed/food_price_merged_clean.csv (final, analysis-ready)\n",
        "\n",
        "## Additional Comments\n",
        "\n",
        "* Join strategy: left join from details → country, prioritising keys ['country','date','item'] then ['country','date'], falling back to ['country'] only if needed.\n",
        "\n",
        "* If your columns differ slightly (e.g., country_name instead of country, month instead of date), the notebook will adapt and log exactly which keys were used.\n",
        "\n",
        "* Currency/unit normalisation can be added as a follow-up step once business rules are agreed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We assume this notebook sits in a subfolder (e.g., jupyter_notebooks/). We make the parent the working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/aminaibrahim/Documents/vscode-projects/food-price-inflation-analysis/jupyter_notebooks'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Access the current directory\n",
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "# Make the parent directory current\n",
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/Users/aminaibrahim/Documents/vscode-projects/food-price-inflation-analysis'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Confirm new current directory\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Section 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Import and Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "country_path = \"data/WLD_RTFP_country_2023-10-02.csv\"\n",
        "details_path = \"data/WLD_RTP_details_2023-10-02.csv\"\n",
        "\n",
        "country_df = pd.read_csv(country_path)\n",
        "details_df = pd.read_csv(details_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Section 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Cleaning Utility Functions\n",
        "\n",
        "The following code defines a set of reusable functions to clean and standardize tabular data using pandas. These functions help ensure consistent column naming, tidy string values, parse dates, convert numeric columns, drop nearly empty columns, and provide a quick summary report. You can use these utilities to prepare raw datasets for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Standardize column names: strip spaces, remove special characters, replace spaces with underscores, and lowercase\n",
        "def normalise_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df.columns = (\n",
        "        df.columns\n",
        "          .str.strip()\n",
        "          .str.replace(r\"[^\\w\\s]\", \"\", regex=True)\n",
        "          .str.replace(r\"\\s+\", \"_\", regex=True)\n",
        "          .str.lower()\n",
        "    )\n",
        "    return df\n",
        "\n",
        "# Clean string columns: strip leading/trailing spaces and collapse multiple spaces into one\n",
        "def tidy_strings(df: pd.DataFrame, cols) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in cols:\n",
        "        if c in df.columns:\n",
        "            df[c] = (df[c].astype(str)\n",
        "                           .str.strip()\n",
        "                           .str.replace(r\"\\s+\", \" \", regex=True))\n",
        "    return df\n",
        "\n",
        "# Parse columns with names like 'date', 'month', 'year', or 'period' into datetime objects\n",
        "def parse_dates(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if re.search(r\"(date|month|year|period)\", c):\n",
        "            try:\n",
        "                df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
        "            except Exception:\n",
        "                pass\n",
        "    return df\n",
        "\n",
        "# Convert columns that look numeric (even if stored as strings) to numeric dtype\n",
        "def coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in df.columns:\n",
        "        if df[c].dtype == object:\n",
        "            sample = df[c].dropna().astype(str).head(50)\n",
        "            looks_numeric = (\n",
        "                not sample.empty and\n",
        "                sample.str.replace(\",\",\"\", regex=False)\n",
        "                      .str.replace(\"%\",\"\", regex=False)\n",
        "                      .str.match(r\"^-?\\d+(\\.\\d+)?$\")\n",
        "                      .mean() > 0.6\n",
        "            )\n",
        "            if looks_numeric:\n",
        "                df[c] = (df[c].astype(str)\n",
        "                               .str.replace(\",\",\"\", regex=False)\n",
        "                               .str.replace(\"%\",\"\", regex=False)\n",
        "                               .str.strip())\n",
        "                df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df\n",
        "\n",
        "# Drop columns that are nearly empty (default: 98% or more missing values)\n",
        "def drop_nearly_empty(df: pd.DataFrame, thresh=0.98) -> pd.DataFrame:\n",
        "    na_ratio = df.isna().mean()\n",
        "    to_drop = na_ratio[na_ratio >= thresh].index.tolist()\n",
        "    return df.drop(columns=to_drop) if to_drop else df\n",
        "\n",
        "# Run all cleaning steps in sequence for a generic DataFrame\n",
        "def clean_generic(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    out = normalise_columns(out)\n",
        "    out = out.drop_duplicates()\n",
        "    # Identify likely categorical columns and clean them\n",
        "    cat_cols = [c for c in out.columns if out[c].dtype == object]\n",
        "    priority = [c for c in [\"country\",\"country_name\",\"item\",\"commodity\",\"product\",\"unit\",\"currency\",\"series\",\"market\"] if c in out.columns]\n",
        "    out = tidy_strings(out, list(dict.fromkeys(priority + cat_cols)))\n",
        "    for c in [\"country\",\"country_name\"]:\n",
        "        if c in out.columns:\n",
        "            out[c] = out[c].str.title()\n",
        "    out = parse_dates(out)\n",
        "    out = coerce_numeric(out)\n",
        "    out = drop_nearly_empty(out, thresh=0.98)\n",
        "    return out\n",
        "\n",
        "# Print a brief summary report of the DataFrame's shape, duplicate count, and top missing columns\n",
        "def brief_report(df: pd.DataFrame, name: str):\n",
        "    print(f\"{name}: shape={df.shape}, duplicates={df.duplicated().sum()}\")\n",
        "    miss = df.isna().sum()\n",
        "    if miss.any():\n",
        "        print(\"  top missing:\", miss.sort_values(ascending=False).head(5).to_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Applying Data Cleaning Functions\n",
        "\n",
        "Now we use the cleaning utilities to process both datasets (`country_df` and `details_df`).  \n",
        "We then print a brief summary report for each cleaned DataFrame to check their shape, duplicate count, and missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Country (clean): shape=(4798, 8), duplicates=0\n",
            "  top missing: {'inflation': 364, 'open': 64, 'high': 64, 'low': 64, 'close': 64}\n",
            "Details (clean): shape=(25, 21), duplicates=0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/09/yd5kcn1539g31p80cnw4dmr80000gn/T/ipykernel_43322/3821305012.py:33: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
            "/var/folders/09/yd5kcn1539g31p80cnw4dmr80000gn/T/ipykernel_43322/3821305012.py:33: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
            "/var/folders/09/yd5kcn1539g31p80cnw4dmr80000gn/T/ipykernel_43322/3821305012.py:33: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
            "/var/folders/09/yd5kcn1539g31p80cnw4dmr80000gn/T/ipykernel_43322/3821305012.py:33: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n"
          ]
        }
      ],
      "source": [
        "# Clean both datasets using the generic cleaning function\n",
        "country_clean = clean_generic(country_df)\n",
        "details_clean = clean_generic(details_df)\n",
        "\n",
        "# Print a summary report for each cleaned DataFrame\n",
        "brief_report(country_clean, \"Country (clean)\")\n",
        "brief_report(details_clean, \"Details (clean)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 4 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving Cleaned Data\n",
        "\n",
        "After cleaning, we save the processed DataFrames as new CSV files in the `data/` directory.  \n",
        "This ensures that the cleaned datasets are available for downstream analysis or sharing, without overwriting the original raw files.  \n",
        "The file names include `_clean` to indicate they have been processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: data/WLD_RTFP_country_2023-10-02_clean.csv\n",
            "Saved: data/WLD_RTP_details_2023-10-02_clean.csv\n"
          ]
        }
      ],
      "source": [
        "# Save cleaned versions alongside the raw files in 'data/'\n",
        "country_out = \"data/WLD_RTFP_country_2023-10-02_clean.csv\"\n",
        "details_out = \"data/WLD_RTP_details_2023-10-02_clean.csv\"\n",
        "\n",
        "country_clean.to_csv(country_out, index=False)\n",
        "details_clean.to_csv(details_out, index=False)\n",
        "\n",
        "print(\"Saved:\", country_out)\n",
        "print(\"Saved:\", details_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Merging Cleaned Datasets\n",
        "\n",
        "To combine the cleaned detail and country datasets, we first ensure both have consistent key columns for merging:\n",
        "- `add_country_key` copies the `country_name` column to `country` if `country` is missing.\n",
        "- `ensure_date` creates a `date` column from alternatives like `month`, `period`, or `year` if needed.\n",
        "\n",
        "We then select the best available join keys from a prioritized list (e.g., `[\"country\",\"date\",\"item\"]`), and perform a left merge from the details to the country data.  \n",
        "This approach ensures robust merging even if column names or structures differ slightly between files.  \n",
        "Finally, we print the merge keys used, the shape of the merged DataFrame, and the number of rows with missing `country` after merging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging on keys: ['country']\n",
            "Merged shape: (4798, 28)\n",
            "Rows with missing `country` after merge: 0\n"
          ]
        }
      ],
      "source": [
        "def add_country_key(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if \"country\" not in df.columns and \"country_name\" in df.columns:\n",
        "        df[\"country\"] = df[\"country_name\"]\n",
        "    return df\n",
        "\n",
        "def ensure_date(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    if \"date\" not in df.columns:\n",
        "        for alt in [\"month\",\"period\",\"year\"]:\n",
        "            if alt in df.columns and pd.api.types.is_datetime64_any_dtype(df[alt]):\n",
        "                df[\"date\"] = df[alt]\n",
        "                break\n",
        "    return df\n",
        "\n",
        "country_keyed = ensure_date(add_country_key(country_clean))\n",
        "details_keyed = ensure_date(add_country_key(details_clean))\n",
        "\n",
        "candidate_key_sets = [\n",
        "    [\"country\",\"date\",\"item\"],\n",
        "    [\"country\",\"date\"],\n",
        "    [\"country\"]\n",
        "]\n",
        "\n",
        "chosen = None\n",
        "for keys in candidate_key_sets:\n",
        "    if all(k in country_keyed.columns for k in keys) and all(k in details_keyed.columns for k in keys):\n",
        "        chosen = keys\n",
        "        break\n",
        "\n",
        "if not chosen:\n",
        "    raise ValueError(\"No suitable join keys found. Inspect columns and adjust `candidate_key_sets`.\")\n",
        "\n",
        "print(\"Merging on keys:\", chosen)\n",
        "\n",
        "merged = pd.merge(\n",
        "    details_keyed,\n",
        "    country_keyed,\n",
        "    on=chosen,\n",
        "    how=\"left\",\n",
        "    suffixes=(\"\", \"_country\")\n",
        ").drop_duplicates()\n",
        "\n",
        "print(\"Merged shape:\", merged.shape)\n",
        "no_match = merged[\"country\"].isna().sum() if \"country\" in merged.columns else 0\n",
        "print(\"Rows with missing `country` after merge:\", int(no_match))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Country duplicate keys: 4773\n",
            "Details duplicate keys: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for duplicate keys before merging\n",
        "print(\"Country duplicate keys:\", country_keyed.duplicated(subset=chosen).sum())\n",
        "print(\"Details duplicate keys:\", details_keyed.duplicated(subset=chosen).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why I Need to Fix Duplicate Keys Before Merging\n",
        "\n",
        "When I see duplicate keys in my `country_keyed` DataFrame, it means that for some join keys, there are multiple matching rows. If I merge without fixing this, each row in `details_keyed` will join with all matching rows in `country_keyed`, which can blow up my merged dataset and introduce errors.\n",
        "\n",
        "To keep my merged data clean and accurate, I need to investigate and resolve these duplicates in `country_keyed` before merging. This might mean dropping exact duplicates, aggregating values, or deciding which record to keep for each key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Country duplicate keys: 4773\n",
            "Details duplicate keys: 0\n",
            "Missing 'country' in details: 0\n",
            "Missing 'country' in country: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for duplicate keys before merging\n",
        "print(\"Country duplicate keys:\", country_keyed.duplicated(subset=chosen).sum())\n",
        "print(\"Details duplicate keys:\", details_keyed.duplicated(subset=chosen).sum())\n",
        "\n",
        "# Check for missing join keys\n",
        "for k in chosen:\n",
        "    print(f\"Missing '{k}' in details:\", details_keyed[k].isna().sum())\n",
        "    print(f\"Missing '{k}' in country:\", country_keyed[k].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resolving Duplicate Keys Before Merging\n",
        "\n",
        "When I check for duplicate keys before merging, I see that there are 4773 duplicate key rows in my `country_keyed` DataFrame and 0 in `details_keyed`. This means that, for my chosen join keys, the country-level data is not unique. If I merge as-is, each row in `details_keyed` will join with all matching rows in `country_keyed`, which could inflate my merged dataset and introduce errors.\n",
        "\n",
        "Additionally, I see there are no missing values for the 'country' key in either DataFrame, so I don't need to worry about missing join keys.\n",
        "\n",
        "Now that I've identified duplicate keys in my `country_keyed` DataFrame, I need to resolve them before merging. This helps prevent data inflation and ensures the merged dataset is accurate.\n",
        "\n",
        "If the duplicate rows are exact matches, I can safely drop them. If they differ, I'll need to review and decide how to aggregate or select a representative row for each key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Country duplicate keys after cleaning: 0\n"
          ]
        }
      ],
      "source": [
        "# Drop exact duplicate rows based on the chosen join keys\n",
        "country_keyed = country_keyed.drop_duplicates(subset=chosen)\n",
        "\n",
        "# Re-check for duplicates to confirm resolution\n",
        "print(\"Country duplicate keys after cleaning:\", country_keyed.duplicated(subset=chosen).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Merged Cleaned File & Quick Quality Assurance\n",
        "\n",
        "Now that I've resolved duplicate keys and completed the merge, I will save the final cleaned dataset to disk.  \n",
        "I'll also run a quick quality assurance check by summarizing numeric columns and showing the most frequent values for key categorical columns.  \n",
        "This helps ensure the merged data is ready for downstream analysis and gives me a first look at the data distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved merged cleaned dataset: data/food_price_merged_cleaned.csv\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>number_of_markets_modeled</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>57.067945</td>\n",
              "      <td>46.429370</td>\n",
              "      <td>9.00</td>\n",
              "      <td>24.000</td>\n",
              "      <td>42.00</td>\n",
              "      <td>79.00</td>\n",
              "      <td>228.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number_of_markets_covered</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>57.067945</td>\n",
              "      <td>46.429370</td>\n",
              "      <td>9.00</td>\n",
              "      <td>24.000</td>\n",
              "      <td>42.00</td>\n",
              "      <td>79.00</td>\n",
              "      <td>228.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>number_of_food_items</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>8.946644</td>\n",
              "      <td>6.629954</td>\n",
              "      <td>3.00</td>\n",
              "      <td>4.000</td>\n",
              "      <td>7.00</td>\n",
              "      <td>11.00</td>\n",
              "      <td>26.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>data_coverage_food</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>34.872926</td>\n",
              "      <td>17.070135</td>\n",
              "      <td>8.84</td>\n",
              "      <td>20.125</td>\n",
              "      <td>31.08</td>\n",
              "      <td>47.97</td>\n",
              "      <td>69.91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>average_annualized_food_inflation</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>11.857870</td>\n",
              "      <td>14.511799</td>\n",
              "      <td>1.24</td>\n",
              "      <td>3.580</td>\n",
              "      <td>6.68</td>\n",
              "      <td>10.79</td>\n",
              "      <td>55.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>maximum_food_drawdown</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>-23.375060</td>\n",
              "      <td>10.914257</td>\n",
              "      <td>-40.67</td>\n",
              "      <td>-32.670</td>\n",
              "      <td>-23.76</td>\n",
              "      <td>-13.96</td>\n",
              "      <td>-2.79</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>average_annualized_food_volatility</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>10.614506</td>\n",
              "      <td>5.290399</td>\n",
              "      <td>1.84</td>\n",
              "      <td>7.150</td>\n",
              "      <td>9.89</td>\n",
              "      <td>12.58</td>\n",
              "      <td>24.77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>average_annual_food_price_correlation_between_markets</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>0.781647</td>\n",
              "      <td>0.171540</td>\n",
              "      <td>0.27</td>\n",
              "      <td>0.670</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.91</td>\n",
              "      <td>0.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>index_confidence_score</th>\n",
              "      <td>4798.0</td>\n",
              "      <td>0.919329</td>\n",
              "      <td>0.039085</td>\n",
              "      <td>0.82</td>\n",
              "      <td>0.890</td>\n",
              "      <td>0.93</td>\n",
              "      <td>0.95</td>\n",
              "      <td>0.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>open</th>\n",
              "      <td>4734.0</td>\n",
              "      <td>1.491880</td>\n",
              "      <td>4.652457</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.740</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.10</td>\n",
              "      <td>102.46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     count       mean  \\\n",
              "number_of_markets_modeled                           4798.0  57.067945   \n",
              "number_of_markets_covered                           4798.0  57.067945   \n",
              "number_of_food_items                                4798.0   8.946644   \n",
              "data_coverage_food                                  4798.0  34.872926   \n",
              "average_annualized_food_inflation                   4798.0  11.857870   \n",
              "maximum_food_drawdown                               4798.0 -23.375060   \n",
              "average_annualized_food_volatility                  4798.0  10.614506   \n",
              "average_annual_food_price_correlation_between_m...  4798.0   0.781647   \n",
              "index_confidence_score                              4798.0   0.919329   \n",
              "open                                                4734.0   1.491880   \n",
              "\n",
              "                                                          std    min     25%  \\\n",
              "number_of_markets_modeled                           46.429370   9.00  24.000   \n",
              "number_of_markets_covered                           46.429370   9.00  24.000   \n",
              "number_of_food_items                                 6.629954   3.00   4.000   \n",
              "data_coverage_food                                  17.070135   8.84  20.125   \n",
              "average_annualized_food_inflation                   14.511799   1.24   3.580   \n",
              "maximum_food_drawdown                               10.914257 -40.67 -32.670   \n",
              "average_annualized_food_volatility                   5.290399   1.84   7.150   \n",
              "average_annual_food_price_correlation_between_m...   0.171540   0.27   0.670   \n",
              "index_confidence_score                               0.039085   0.82   0.890   \n",
              "open                                                 4.652457   0.01   0.740   \n",
              "\n",
              "                                                      50%    75%     max  \n",
              "number_of_markets_modeled                           42.00  79.00  228.00  \n",
              "number_of_markets_covered                           42.00  79.00  228.00  \n",
              "number_of_food_items                                 7.00  11.00   26.00  \n",
              "data_coverage_food                                  31.08  47.97   69.91  \n",
              "average_annualized_food_inflation                    6.68  10.79   55.30  \n",
              "maximum_food_drawdown                              -23.76 -13.96   -2.79  \n",
              "average_annualized_food_volatility                   9.89  12.58   24.77  \n",
              "average_annual_food_price_correlation_between_m...   0.82   0.91    0.99  \n",
              "index_confidence_score                               0.93   0.95    0.99  \n",
              "open                                                 0.96   1.10  102.46  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top values for `country`:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "country\n",
              "Afghanistan    202\n",
              "Somalia        202\n",
              "Nigeria        202\n",
              "Niger          202\n",
              "Mozambique     202\n",
              "Mali           202\n",
              "Liberia        202\n",
              "Burundi        202\n",
              "Lao Pdr        202\n",
              "South Sudan    202\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top values for `currency`:\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "currency\n",
              "XOF    808\n",
              "XAF    760\n",
              "AFN    202\n",
              "SOS    202\n",
              "CDF    202\n",
              "GMD    202\n",
              "HTG    202\n",
              "SSP    202\n",
              "LAK    202\n",
              "BIF    202\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Save the merged cleaned dataset\n",
        "merged_out = \"data/food_price_merged_cleaned.csv\"\n",
        "merged.to_csv(merged_out, index=False)\n",
        "print(\"Saved merged cleaned dataset:\", merged_out)\n",
        "\n",
        "# Quick numeric summary (first 10 rows of summary only to keep output tidy)\n",
        "import numpy as np\n",
        "num_summary = merged.select_dtypes(include=[np.number]).describe().T.head(10)\n",
        "display(num_summary)\n",
        "\n",
        "# Frequent categories to guide EDA\n",
        "for key in [\"country\", \"item\", \"unit\", \"currency\", \"series\"]:\n",
        "    if key in merged.columns:\n",
        "        print(f\"\\nTop values for `{key}`:\")\n",
        "        display(merged[key].value_counts().head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Section 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Engineering\n",
        "\n",
        "Now that I have a clean, merged dataset, I can begin feature engineering to create new variables that may improve analysis and modeling. Feature engineering helps extract more information from the data and can reveal important trends or relationships.\n",
        "\n",
        "**Possible feature engineering steps:**\n",
        "- Create time-based features (e.g., year, month, quarter) from date columns.\n",
        "- Calculate percentage change or rolling averages for price or inflation columns.\n",
        "- Encode categorical variables (e.g., one-hot encoding for `country`, `item`, or `unit`).\n",
        "- Create flags for missing or extreme values.\n",
        "- Aggregate data at different levels (e.g., country-level monthly averages).\n",
        "\n",
        "Below, I demonstrate some initial feature engineering steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1 Extract year and month from a date column\n",
        "\n",
        "If date exists, split into numeric year and month for seasonal/time-based analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added year and month columns from date.\n"
          ]
        }
      ],
      "source": [
        "if \"date\" in merged.columns and pd.api.types.is_datetime64_any_dtype(merged[\"date\"]):\n",
        "    merged[\"year\"] = merged[\"date\"].dt.year\n",
        "    merged[\"month\"] = merged[\"date\"].dt.month\n",
        "    print(\"Added year and month columns from date.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 Calculate month-on-month percentage change for close price\n",
        "\n",
        "Tracks short-term price momentum per country."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added month-on-month close percentage change.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/09/yd5kcn1539g31p80cnw4dmr80000gn/T/ipykernel_43322/1366236795.py:3: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
            "  merged[\"close_pct_change\"] = merged.groupby(\"country\")[\"close\"].pct_change()\n"
          ]
        }
      ],
      "source": [
        "if {\"country\", \"date\", \"close\"}.issubset(merged.columns):\n",
        "    merged = merged.sort_values([\"country\", \"date\"])\n",
        "    merged[\"close_pct_change\"] = merged.groupby(\"country\")[\"close\"].pct_change()\n",
        "    print(\"Added month-on-month close percentage change.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3 Calculate a 3-month rolling average for close\n",
        "\n",
        "Smooths price fluctuations for trend analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 3-month rolling average for close price.\n"
          ]
        }
      ],
      "source": [
        "if {\"country\", \"close\"}.issubset(merged.columns):\n",
        "    merged[\"close_rolling_avg_3\"] = (\n",
        "        merged.groupby(\"country\")[\"close\"]\n",
        "              .transform(lambda x: x.rolling(window=3, min_periods=1).mean())\n",
        "    )\n",
        "    print(\"Added 3-month rolling average for close price.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.4 Calculate year-on-year percentage change for close\n",
        "\n",
        "Highlights long-term trends by comparing to the same month last year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added year-on-year close percentage change.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/09/yd5kcn1539g31p80cnw4dmr80000gn/T/ipykernel_43322/852167951.py:2: FutureWarning: The default fill_method='ffill' in SeriesGroupBy.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
            "  merged[\"close_yoy_change\"] = merged.groupby(\"country\")[\"close\"].pct_change(periods=12)\n"
          ]
        }
      ],
      "source": [
        "if {\"country\", \"date\", \"close\"}.issubset(merged.columns):\n",
        "    merged[\"close_yoy_change\"] = merged.groupby(\"country\")[\"close\"].pct_change(periods=12)\n",
        "    print(\"Added year-on-year close percentage change.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.5 One-hot encode the item column\n",
        "\n",
        "Converts item categories into separate binary columns for modelling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "if \"item\" in merged.columns:\n",
        "    merged = pd.get_dummies(merged, columns=[\"item\"], prefix=\"item\")\n",
        "    print(\"One-hot encoded the item column.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.6 Create inflation bands\n",
        "\n",
        "Groups inflation values into named categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added inflation bands.\n"
          ]
        }
      ],
      "source": [
        "if \"inflation\" in merged.columns:\n",
        "    bins = [-float(\"inf\"), 0, 5, 10, float(\"inf\")]\n",
        "    labels = [\"deflation\", \"low\", \"medium\", \"high\"]\n",
        "    merged[\"inflation_band\"] = pd.cut(merged[\"inflation\"], bins=bins, labels=labels)\n",
        "    print(\"Added inflation bands.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.7 Calculate average close price per country-item\n",
        "\n",
        "Captures cross-category pricing patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "if {\"country\", \"item\", \"close\"}.issubset(merged.columns):\n",
        "    merged[\"country_item_avg_close\"] = (\n",
        "        merged.groupby([\"country\", \"item\"])[\"close\"].transform(\"mean\")\n",
        "    )\n",
        "    print(\"Added average close price per country-item.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.8 Calculate days since last observation\n",
        "\n",
        "Flags irregular time gaps in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added days since last observation.\n"
          ]
        }
      ],
      "source": [
        "if {\"country\", \"date\"}.issubset(merged.columns) and pd.api.types.is_datetime64_any_dtype(merged[\"date\"]):\n",
        "    merged = merged.sort_values([\"country\", \"date\"])\n",
        "    merged[\"days_since_last\"] = merged.groupby(\"country\")[\"date\"].diff().dt.days\n",
        "    print(\"Added days since last observation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.9 Flag missing inflation values\n",
        "\n",
        "Allows models to consider “missingness” as a potential signal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added missing value flag for inflation.\n"
          ]
        }
      ],
      "source": [
        "if \"inflation\" in merged.columns:\n",
        "    merged[\"inflation_missing\"] = merged[\"inflation\"].isna().astype(int)\n",
        "    print(\"Added missing value flag for inflation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving the Feature Engineered Dataset\n",
        "\n",
        "After creating new features to enhance my analysis (such as extracting year and month, calculating percentage changes, rolling averages, one-hot encoding, and more), I save the updated DataFrame to a new CSV file called `food_price_feature_engineered.csv`.  \n",
        "This ensures that all engineered features are preserved and ready for further analysis, modeling, or sharing with others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature engineered dataset saved to: data/food_price_feature_engineered.csv\n"
          ]
        }
      ],
      "source": [
        "# Save feature engineered dataset\n",
        "feature_engineered_path = \"data/food_price_feature_engineered.csv\"\n",
        "merged.to_csv(feature_engineered_path, index=False)\n",
        "print(f\"Feature engineered dataset saved to: {feature_engineered_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion & Summary\n",
        "\n",
        "In this notebook, I have taken raw food price inflation data and transformed it into a clean, analysis-ready dataset through a series of structured ETL (Extract, Transform, Load) steps:\n",
        "\n",
        "- **Data Cleaning:** Loaded the raw CSV files, standardized column names, parsed dates, coerced numeric fields, and harmonized key categorical variables.\n",
        "- **Quality Assurance:** Checked for duplicates and missing values, ensuring the integrity of the cleaned data.\n",
        "- **Merging:** Combined country-level and item-level datasets using robust join keys, resolving duplicate key issues to prevent data inflation.\n",
        "- **Feature Engineering:** Created new variables such as time-based features, percentage changes, rolling averages, one-hot encodings, and missing value flags to enrich the dataset for analysis and modeling.\n",
        "- **Saving Outputs:** Saved both the cleaned and feature-engineered datasets for downstream analysis.\n",
        "\n",
        "This process ensures that the final dataset is tidy, consistent, and rich in features, making it suitable for exploratory data analysis, visualization, and predictive modeling. You can now confidently use `food_price_feature_engineered.csv` for further insights and machine learning tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
